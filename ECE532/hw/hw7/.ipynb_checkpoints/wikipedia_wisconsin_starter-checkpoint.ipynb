{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import eigs\n",
    "\n",
    "edges_file = open('wisconsin_edges.csv', \"r\")\n",
    "nodes_file = open('wisconsin_nodes.csv', \"r\")\n",
    "\n",
    "# create a dictionary where nodes_dict[i] = name of wikipedia page\n",
    "nodes_dict = {}\n",
    "for line in nodes_file:\n",
    "    nodes_dict[int(line.split(',',1)[0].strip())] = line.split(',',1)[1].strip()\n",
    "\n",
    "node_count = len(nodes_dict)\n",
    "\n",
    "# create adjacency matrix\n",
    "A = np.zeros((node_count, node_count))\n",
    "for line in edges_file:\n",
    "    from_node = int(line.split(',')[0].strip())\n",
    "    to_node = int(line.split(',')[1].strip())\n",
    "    A[to_node, from_node] = 1.0\n",
    "\n",
    "## Add code below to (1) prevent traps and (2) find the most important pages     \n",
    "# Hint -- instead of computing the entire eigen-decomposition of a matrix X using\n",
    "# s, E = np.linalg.eig(A)\n",
    "# you can compute just the first eigenvector with:\n",
    "# s, E = eigs(csc_matrix(A), k = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Wikipedia Wisconsin PageRank.  \n",
    "\n",
    "A dataset was created by looking at the links between Wikipedia pages with the word ‘Wisconsin’ in the title.  The corresponding graph  contains  5482  nodes  (Wikipedia  pages)  and  41,306  edges  (links  between  the pages).Two files contain the data.  The edges are store in a file with two columns of integers,where the first column indicates the from node and the second column indicates the to node.  A second file contains the titles of the Wikipedia pages, and their integer value.Use the starter script to load the edges file and create an adjacency matrix A, where $A_{ij}$ = 1 if there is a edge from node $j$ to node $i$, and zero elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Write code to:\n",
    "\n",
    "(i) Ensure there are no traps by adding 0.001 to each entry of $A$.\n",
    "\n",
    "(ii)  normalize $A$ , and \n",
    "\n",
    "(iii) use an eigen decomposition to rank the importance of the Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A += 0.001\n",
    "\n",
    "for k in range(A.shape[0]):\n",
    "    norm = np.sum(A[:,k])\n",
    "    A[:,k] = A[:,k] / norm\n",
    "    \n",
    "s, E = eigs(csc_matrix(A), k = 1)\n",
    "\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  b) What is the title of the page ranked 1st (i.e, the most important page)?\n",
    "\n",
    ": Page 5089."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Wisconsin\"'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(E == np.amax(E))\n",
    "nodes_dict[5089]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) What is the title of the page ranked 3rd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Madison, Wisconsin\"'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(E == np.partition(E.flatten(), -2)[-3])\n",
    "nodes_dict[1345]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent and Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary linear classifier, the predicted class is given as $\\hat{y_i}= sign(x^T_iw)$.  \n",
    "Trainingin machine learning involves finding a $w$ that does a good job on labeled data,  and \n",
    "often involves solving an optimization of the form: min \\Sigma _i l`i(w) +λr(w) where`i(w) is the loss on the ith training example, andr(w) is a regularizer.  In ridgeregression, the loss function is the squared error and the regularizer is the 2-norm ofw, and training amounts to solving the following minimization:minwn∑i=1(xTiw−yi)2+λ||w||22.For some classification tasks, the squared error can be a poor loss function, since easy-to-classify  data  points  can  have  a  large  associated  loss.   This  happens  when  a  datapoint is correctly classified, but far from the decision boundary (i.e, the absolute valueofxTiwis large).In practice, an often more appropriate loss function is the logistic loss, given by`i(w) = log(1 +e−yixTiw)1 of 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
